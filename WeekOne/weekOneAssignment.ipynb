{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "582e72f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Markdown, display ,update_display\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbcc34b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are like best professor who can explain topics in a very detailed way with proper examples and take feedback from the user to improve the explanation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d54733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_topic(topic):\n",
    "    user_prompt=f\"Explain the topic: {topic} . I want to learn it in very detailed way as i couldnt understand it from my tutor\"\n",
    "    response =client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\":system_prompt},\n",
    "            {\"role\":\"user\",\"content\":user_prompt}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    responses=\"\"\n",
    "    for chunk in response:\n",
    "        responses += chunk.choices[0].delta.content or ''\n",
    "        update_display(Markdown(responses), display_id=display_handle.display_id)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdf84abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Absolutely! I’d be happy to help you understand Transformers in machine learning. We’ll break it down into manageable parts, cover the core concepts, components, and some tangible examples. Feel free to provide feedback or ask for clarification on any points as we progress!\n",
       "\n",
       "### What are Transformers?\n",
       "\n",
       "Transformers are a type of model architecture used primarily for natural language processing (NLP) tasks, but their application has expanded to various domains, including computer vision and reinforcement learning. Introduced in the revolutionary paper **\"Attention is All You Need\"** by Vaswani et al. in 2017, transformers have become the backbone of many modern machine learning models, including BERT, GPT, and many others.\n",
       "\n",
       "### Core Components of Transformers\n",
       "\n",
       "1. **Input Representation**:\n",
       "   - Transformers process sequences of data. For text, this means converting words into vectors using embeddings (e.g., Word2Vec or GloVe).\n",
       "   - Each input word is represented as a vector in a high-dimensional space.\n",
       "   - Additionally, positional encoding is added to the input embeddings to give the model information about the order of the words in the sequence, as transformers do not have any inherent sense of order.\n",
       "\n",
       "2. **Self-Attention Mechanism**:\n",
       "   - The self-attention mechanism is the heart of the transformer architecture. It allows the model to weigh the relevance of different words in a sequence when processing a specific word.\n",
       "   - Each input vector is transformed into three vectors: **Query (Q)**, **Key (K)**, and **Value (V)** through learned linear projections.\n",
       "   - The attention score, which indicates the importance of a word in relation to another, is computed using the dot product of the Query and Key vectors followed by a softmax operation.\n",
       "   - Mathematically, for word \\( i \\) and word \\( j \\):\n",
       "     \\[\n",
       "     \\text{Attention}(Q_i, K_j) = \\frac{\\exp(Q_i \\cdot K_j)}{\\sum_{k}\\exp(Q_i \\cdot K_k)}\n",
       "     \\]\n",
       "   - The final output for each word is computed as a weighted sum of the Value vectors, where the weights are the attention scores.\n",
       "\n",
       "3. **Multi-Head Attention**:\n",
       "   - Instead of having a single attention mechanism, transformers apply multiple attention heads in parallel. This allows the model to capture different types of relationships and interactions across different sub-spaces.\n",
       "   - The outputs from all the heads are then concatenated and linearly transformed to produce the final output.\n",
       "\n",
       "4. **Feed-Forward Neural Networks**:\n",
       "   - After the multi-head attention, the output is fed into a feed-forward neural network (FFN) that processes the information independently for each position in the sequence.\n",
       "   - The feed-forward network typically consists of two linear transformations with a non-linear activation function (like ReLU) in between.\n",
       "\n",
       "5. **Layer Normalization and Residual Connections**:\n",
       "   - Each sub-layer (like attention or FFN) in the transformer includes a residual connection around it, which helps in stabilizing the training of deep networks.\n",
       "   - Layer normalization is also applied after each of these sub-layers to help with the optimization of the network.\n",
       "\n",
       "6. **Stacking Layers**:\n",
       "   - The previous components are typically stacked several times to form what is known as the **encoder** and **decoder** architecture. In many implementations focused solely on language understanding tasks (like BERT), only the encoder part is used.\n",
       "\n",
       "### Encoder-Decoder Architecture\n",
       "\n",
       "- The original transformer architecture comprises two components:\n",
       "  1. **Encoder**: This takes the input sequence and processes it to generate a set of context-sensitive embeddings for each input position.\n",
       "  2. **Decoder**: This takes the encoder's output, along with the previously generated tokens (during training), and produces the output sequence.\n",
       "\n",
       "### Example: Language Translation\n",
       "\n",
       "Consider a simple example of translating an English sentence “The cat sits on the mat” to French:\n",
       "\n",
       "1. **Input Representation**: The sentence is tokenized, and each token is converted into an embedding.\n",
       "2. **Self-Attention**: While processing the token \"cat,\" the self-attention mechanism will pay attention to other tokens like “the,” “sits,” and “on” to understand their relationship and context.\n",
       "3. **Multi-Head Attention**: During this process, different heads might focus on different kinds of relationships (e.g., one head might focus on grammatical relationships while another looks at semantic relationships).\n",
       "4. **Output Generation**: The decoder takes the encoder's output and uses it to generate the French output, “Le chat est sur le tapis.”\n",
       "\n",
       "### Why Transformers?\n",
       "\n",
       "- **Parallelization**: Unlike RNNs, transformers can process all tokens in a sequence simultaneously, leading to faster training.\n",
       "- **Long-Range Dependencies**: They handle long-range dependencies better, which is critical for understanding context in language tasks.\n",
       "- **Flexibility**: The architecture can be adapted for various tasks beyond NLP, including image processing and reinforcement learning.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "Transformers have revolutionized how we approach many tasks in machine learning, particularly in natural language processing. By utilizing self-attention, they have solved many of the limitations found in traditional models.\n",
       "\n",
       "### Feedback and Questions\n",
       "\n",
       "Is there any specific section you would like me to explain further? Or do you have any examples that you’re interested in? Please let me know!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explain_topic(\"Transformers in ML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a7800c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
