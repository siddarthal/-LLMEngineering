{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "582e72f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Markdown, display ,update_display\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbcc34b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are like best AIML professor who can explain topics in a very detailed way with proper examples \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d54733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_topic(topic):\n",
    "    user_prompt=f\"Explain the topic: {topic} . I want to learn it in very detailed way as i couldnt understand it from my tutor\"\n",
    "    response =client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\":system_prompt},\n",
    "            {\"role\":\"user\",\"content\":user_prompt}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    responses=\"\"\n",
    "    for chunk in response:\n",
    "        responses += chunk.choices[0].delta.content or ''\n",
    "        update_display(Markdown(responses), display_id=display_handle.display_id)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdf84abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Absolutely! Let's dive into Transformers in Machine Learning (ML) step by step, starting from the basics and gradually moving on to advanced concepts. Please feel free to ask questions at any point or let me know if you'd like me to clarify any part.\n",
       "\n",
       "### Overview of Transformers\n",
       "\n",
       "Transformers are a type of neural network architecture introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. They have become the foundation for many state-of-the-art models in natural language processing (NLP), such as BERT, GPT, and T5.\n",
       "\n",
       "### Key Concepts\n",
       "\n",
       "1. **Attention Mechanism**: \n",
       "   - At the core of the Transformer architecture is the attention mechanism. It allows the model to weigh the importance of different words in a sentence, irrespective of their position.\n",
       "   - **Self-attention** is when a word pays attention to all other words in the same input sequence.\n",
       "   - For example, in the sentence \"The cat sat on the mat,\" when processing the word \"sat,\" the model can pay attention to \"cat\" to understand who is performing the action.\n",
       "\n",
       "2. **Multi-Head Attention**:\n",
       "   - Instead of having a single attention mechanism, the Transformer uses multiple heads (multi-head attention). Each head looks at different parts of the input and captures various aspects of the relationships between words.\n",
       "   - For instance, one head might focus on local syntactic relationships, while another may capture broader context.\n",
       "\n",
       "3. **Positional Encoding**:\n",
       "   - Unlike recurrent neural networks (RNNs), Transformers do not process data sequentially. Therefore, they need to encode the position of words in the sequence to retain information about word order.\n",
       "   - Positional encoding adds a unique vector to each word's embedding based on its position, helping the model to understand the order of tokens (words).\n",
       "\n",
       "4. **Encoder and Decoder**:\n",
       "   - The Transformer architecture consists of two main components: the **Encoder** and the **Decoder**.\n",
       "     - **Encoder**: It takes the input sequence (e.g., a sentence) and processes it into feature representations. It comprises multiple stacked layers, each containing a multi-head attention mechanism and a feedforward neural network.\n",
       "     - **Decoder**: It generates the output sequence (e.g., a translated sentence) from the encoded representations. Like the encoder, it also consists of multiple layers, but it includes additional attention mechanisms to focus on the encoder's output.\n",
       "\n",
       "### Transformer Architecture\n",
       "\n",
       "Hereâ€™s a simplified view of the Transformer architecture:\n",
       "\n",
       "1. **Input Encoding**: Each word in the input sequence is represented as an embedding vector.\n",
       "2. **Add Positional Encoding**: Positional encodings are added to the embeddings to incorporate information about the word position.\n",
       "3. **Stacks of Encoder Layers**: Each encoder layer consists of two main components:\n",
       "   - Multi-Head Self-Attention\n",
       "   - Feedforward Neural Network\n",
       "   - These layers help with capturing contextual information and transforming the input representations.\n",
       "4. **Stacks of Decoder Layers**: Each decoder layer is similar but also includes mechanisms to attend to the encoder's output.\n",
       "5. **Output Layer**: Finally, a softmax layer generates probabilities for each possible output token.\n",
       "\n",
       "### Applications of Transformers\n",
       "\n",
       "Transformers have revolutionized many areas of machine learning, not just NLP. Here are some applications:\n",
       "\n",
       "- **Natural Language Understanding**: Tasks like sentiment analysis, question answering, and language modeling.\n",
       "- **Natural Language Generation**: Generating coherent text, such as in chatbots or story generation.\n",
       "- **Machine Translation**: Converting text from one language to another effectively.\n",
       "- **Image Processing**: Vision Transformers (ViTs) have been developed to analyze images, treating image patches similarly to text tokens.\n",
       "\n",
       "### Example: BERT (Bidirectional Encoder Representations from Transformers)\n",
       "\n",
       "BERT is a specific implementation of the Transformer made particularly powerful for understanding language. It uses the encoder part of the Transformer and is trained using two main tasks:\n",
       "\n",
       "- **Masked Language Modeling**: Randomly masks words in a sentence and trains the model to predict these masked words based on their context.\n",
       "- **Next Sentence Prediction**: Trains the model to understand sentence relationships by predicting if a sentence follows another.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "Transformers have shifted the focus in ML towards architectures that can leverage parallelization, leverage attention mechanisms, and process sequences more fluidly than previous architectures like RNNs and LSTMs. Their versatility means they are becoming standard tools in both NLP and beyond.\n",
       "\n",
       "### Feedback and Questions\n",
       "\n",
       "Does this explanation help clarify the Transformer architecture and its components for you? Are there any specific parts you'd like me to go into more detail about, or any examples you want expanded?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explain_topic(\"Transformers in ML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4a7800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llamaclient = OpenAI(base_url=\"http://localhost:11434/v1\",api_key=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23a5334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_response_for_question(topic):\n",
    "    user_prompt=f\"Explain the topic: {topic} . I want to learn it in very detailed way as i couldnt understand it from my tutor give response in markdown format\"\n",
    "    response = llamaclient.chat.completions.create(\n",
    "        model=\"codellama:7b\",\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\":system_prompt},\n",
    "            {\"role\":\"user\",\"content\":\"\"}\n",
    "        ],\n",
    "    )\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7cbdd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-92', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Greetings! I'm here to help you understand complex concepts and explain them in detail. My primary goal is to make sure every student has a smooth learning experience. \\n\\nI would be glad to hear from you, please let me know some of the challenging topics you need assistance with.\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1764177773, model='codellama:7b', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=63, prompt_tokens=40, total_tokens=103, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "llama_response_for_question(\"Transformers in ML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61f1aa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def llama_response_for_question(topic):\n",
    "    user_prompt=f\"Explain the topic: {topic} . I want to learn it in very detailed way as i couldnt understand it from my tutor give response in markdown format\"\n",
    "    messages=[{\"role\":\"system\",\"content\":system_prompt},{\"role\":\"user\",\"content\":user_prompt}]\n",
    "    response = ollama.chat(model=\"codellama:7b\", messages=messages)\n",
    "    reply = response['message']['content']\n",
    "    display(Markdown(reply))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbaa0f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Introduction\n",
       "------------\n",
       "\n",
       "* Transformers are a type of neural network architecture that have gained widespread popularity in recent years due to their effectiveness in various natural language processing tasks.\n",
       "* The key feature of transformer models is the self-attention mechanism, which allows them to model complex relationships between different parts of the input sequence.\n",
       "* In this response, I will provide a detailed explanation of transformers in machine learning, including their architecture, variants, and applications.\n",
       "\n",
       "Architecture\n",
       "------------\n",
       "\n",
       "* The transformer architecture consists of an encoder and a decoder.\n",
       "* The encoder takes in a sequence of tokens (e.g., words or characters) and outputs a continuous representation of the input sequence.\n",
       "* The decoder then takes this continuous representation as input and generates a sequence of output tokens.\n",
       "* Both the encoder and decoder are composed of multiple identical layers, each of which consists of a self-attention mechanism followed by a feed-forward neural network (FFNN).\n",
       "* The self-attention mechanism allows the model to attend to all positions in the input sequence when computing the output for a particular position.\n",
       "* The FFNN processes the output of the self-attention mechanism and generates the final output for that position.\n",
       "\n",
       "Variants\n",
       "---------\n",
       "\n",
       "* There are several variants of transformer models, each with its own strengths and weaknesses.\n",
       "* Some popular variants include:\n",
       "\t+ BERT (Bidirectional Encoder Representations from Transformers): a pre-trained transformer model that has been trained on a large corpus of text and can be fine-tuned for various NLP tasks.\n",
       "\t+ RoBERTa (Robustly Optimized BERT Pretraining Approach): a variant of BERT that uses a different approach to training and has been shown to perform well on longer sequences.\n",
       "\t+ DistilBERT (Distilled BERT): a smaller and more efficient version of BERT that has been trained to match the performance of the full BERT model while requiring fewer computational resources.\n",
       "\n",
       "Applications\n",
       "-------------\n",
       "\n",
       "* Transformers have a wide range of applications in natural language processing, including:\n",
       "\t+ Language translation: transformer models can be used to translate text from one language to another.\n",
       "\t+ Text generation: transformer models can be used to generate new text that is similar to a given input text.\n",
       "\t+ Question answering: transformer models can be used to answer questions based on the information contained in a piece of text.\n",
       "\t+ Sentiment analysis: transformer models can be used to classify text as positive, negative, or neutral based on its sentiment.\n",
       "\n",
       "Conclusion\n",
       "----------\n",
       "\n",
       "* Transformers are a powerful tool for natural language processing tasks and have been shown to achieve state-of-the-art results in many applications.\n",
       "* The self-attention mechanism allows transformer models to model complex relationships between different parts of the input sequence, making them well-suited for tasks that require understanding the context and meaning of text.\n",
       "* There are many variants of transformer models available, each with its own strengths and weaknesses, and they have been applied to a wide range of natural language processing tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama_response_for_question(\"Transformers in ML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b2c8a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
